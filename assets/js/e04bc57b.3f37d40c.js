"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[1749],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>u});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),c=p(n),u=r,h=c["".concat(s,".").concat(u)]||c[u]||d[u]||o;return n?a.createElement(h,i(i({ref:t},m),{},{components:n})):a.createElement(h,i({ref:t},m))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},9272:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const o={},i="Distributed Computing",l={unversionedId:"scaling/remote-tasks/distributed-computing",id:"scaling/remote-tasks/distributed-computing",title:"Distributed Computing",description:"Metaflow's foreach construct allows you to run tasks concurrently.",source:"@site/docs/scaling/remote-tasks/distributed-computing.md",sourceDirName:"scaling/remote-tasks",slug:"/scaling/remote-tasks/distributed-computing",permalink:"/scaling/remote-tasks/distributed-computing",draft:!1,editUrl:"https://github.dev/Netflix/metaflow-docs/blob/master/docs/scaling/remote-tasks/distributed-computing.md",tags:[],version:"current",frontMatter:{},sidebar:"python",previous:{title:"Installing Drivers and Frameworks",permalink:"/scaling/remote-tasks/installing-drivers-and-frameworks"},next:{title:"Using Kubernetes",permalink:"/scaling/remote-tasks/kubernetes"}},s={},p=[{value:"High-level decorators",id:"high-level-decorators",level:2},{value:"Low-level access",id:"low-level-access",level:2},{value:"MPI example",id:"mpi-example",level:3}],m={toc:p};function d(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"distributed-computing"},"Distributed Computing"),(0,r.kt)("p",null,"Metaflow's ",(0,r.kt)("a",{parentName:"p",href:"/metaflow/basics#foreach"},(0,r.kt)("inlineCode",{parentName:"a"},"foreach")," construct")," allows you to run tasks concurrently.\nIn the case ",(0,r.kt)("inlineCode",{parentName:"p"},"foreach"),", tasks execute independently. This pattern works well when the workload\nis ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Embarrassingly_parallel"},(0,r.kt)("em",{parentName:"a"},"embarrassingly parallel")),", that is,\ntasks don't communicate with each other and they don't have to execute simultaneously."),(0,r.kt)("p",null,"There are other workloads, such as distributed training of large models, which require\ntasks to interact with each other. Metaflow provides another mechanism, the ",(0,r.kt)("inlineCode",{parentName:"p"},"@parallel")," decorator,\nwhich orchestrates such inter-dependent tasks. In effect, the decorator launches\nan ephemeral compute cluster on the fly, as a part of a Metaflow flow, benefiting from\nMetaflow features like ",(0,r.kt)("a",{parentName:"p",href:"/scaling/dependencies"},"dependency management"),",\n",(0,r.kt)("a",{parentName:"p",href:"/scaling/tagging"},"versioning"),", and ",(0,r.kt)("a",{parentName:"p",href:"/production/introduction"},"production deployments"),"."),(0,r.kt)("p",null,"Typically, this pattern is used through one of the framework-specific decorators like ",(0,r.kt)("inlineCode",{parentName:"p"},"@torchrun"),"\nor ",(0,r.kt)("inlineCode",{parentName:"p"},"@deepspeed"),", described below, which make it easy to use a particular framework for distributed\ntraining. If you need low-level access to the cluster, e.g. to use it with a framework that doesn't\nhave a corresponding high-level decorator yet, see documentation for low-level access at the\nend of this page."),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"To use distributed computing, follow ",(0,r.kt)("a",{parentName:"p",href:"https://outerbounds.com/engineering/operations/distributed-computing/"},"set up instructions\nhere"),". If you need\nhelp getting started, contact ",(0,r.kt)("a",{parentName:"p",href:"http://slack.metaflow.org"},"Metaflow Slack"),".")),(0,r.kt)("h2",{id:"high-level-decorators"},"High-level decorators"),(0,r.kt)("p",null,"The easiest way to get started is to use one of the high-level decorators - ",(0,r.kt)("a",{parentName:"p",href:"https://outerbounds.com/blog/distributed-training-with-metaflow/"},"see an overview\nin this blog post"),":"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"center"},"Decorator Implementation"),(0,r.kt)("th",{parentName:"tr",align:null},"UX"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:"center"},"PyPi Release"),(0,r.kt)("th",{parentName:"tr",align:"center"},"Example"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-torchrun"},(0,r.kt)("inlineCode",{parentName:"a"},"@torchrun"))),(0,r.kt)("td",{parentName:"tr",align:null},"Use ",(0,r.kt)("inlineCode",{parentName:"td"},"current.torch.run")," to submit your ",(0,r.kt)("inlineCode",{parentName:"td"},"torch.distributed")," program. No need to log into each node, call the code once in ",(0,r.kt)("inlineCode",{parentName:"td"},"@step"),"."),(0,r.kt)("td",{parentName:"tr",align:null},"A ",(0,r.kt)("a",{parentName:"td",href:"https://pytorch.org/docs/stable/elastic/run.html"},(0,r.kt)("inlineCode",{parentName:"a"},"torchrun"))," command that runs ",(0,r.kt)("inlineCode",{parentName:"td"},"@step")," function code on each node. ",(0,r.kt)("a",{parentName:"td",href:"https://pytorch.org/tutorials/beginner/dist_overview.html"},"Torch distributed")," is used under the hood to handle communication between nodes."),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://pypi.org/project/metaflow-torchrun/"},(0,r.kt)("inlineCode",{parentName:"a"},"metaflow-torchrun"))),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-torchrun/blob/main/examples/min-gpt/flow.py"},"MinGPT"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-deepspeed"},(0,r.kt)("inlineCode",{parentName:"a"},"@deepspeed"))),(0,r.kt)("td",{parentName:"tr",align:null},"Exposes ",(0,r.kt)("inlineCode",{parentName:"td"},"current.deepspeed.run")," ",(0,r.kt)("br",null)," Requires OpenSSH and OpenMPI installed in the Metaflow task container."),(0,r.kt)("td",{parentName:"tr",align:null},"Form MPI cluster with passwordless SSH configured at task runtime (to reduce the risk of leaking private keys).  Submit the Deepspeed program and run."),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://pypi.org/project/metaflow-deepspeed/"},(0,r.kt)("inlineCode",{parentName:"a"},"metaflow-deepspeed"))),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-deepspeed/tree/main/examples/bert"},"Bert")," & ",(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-deepspeed/tree/main/examples/dolly"},"Dolly"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-ray/tree/main"},(0,r.kt)("inlineCode",{parentName:"a"},"@metaflow_ray"))),(0,r.kt)("td",{parentName:"tr",align:null},"Write a Ray program locally or call script from ",(0,r.kt)("inlineCode",{parentName:"td"},"@step")," function, ",(0,r.kt)("inlineCode",{parentName:"td"},"@metaflow_ray")," takes care of forming the Ray cluster."),(0,r.kt)("td",{parentName:"tr",align:null},"Forms a ",(0,r.kt)("a",{parentName:"td",href:"https://docs.ray.io/en/latest/cluster/getting-started.html"},"Ray cluster")," dynamically. Runs the ",(0,r.kt)("inlineCode",{parentName:"td"},"@step")," function code on the control task as Ray\u2019s \u201chead node\u201d."),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://pypi.org/project/metaflow-ray/"},(0,r.kt)("inlineCode",{parentName:"a"},"metaflow-ray"))),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-ray/tree/main/examples/ray-fine-tuning-gpt-j"},"GPT-J")," & ",(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-ray/tree/main/examples/train"},"Distributed XGBoost"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-tensorflow/tree/main"},(0,r.kt)("inlineCode",{parentName:"a"},"@tensorflow"))),(0,r.kt)("td",{parentName:"tr",align:null},"Put TensorFlow code in a distributed strategy scope, and call it from step function."),(0,r.kt)("td",{parentName:"tr",align:null},"Run the ",(0,r.kt)("inlineCode",{parentName:"td"},"@step")," function code on each node. This means the user picks the appropriate ",(0,r.kt)("a",{parentName:"td",href:"https://www.tensorflow.org/guide/distributed_training#types_of_strategies"},"strategy")," in their code."),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://pypi.org/project/metaflow-tensorflow/"},(0,r.kt)("inlineCode",{parentName:"a"},"metaflow-tensorflow"))),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-tensorflow/tree/main/examples/multi-node"},"Keras Distributed"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-mpi"},(0,r.kt)("inlineCode",{parentName:"a"},"@mpi"))),(0,r.kt)("td",{parentName:"tr",align:null},"Exposes ",(0,r.kt)("inlineCode",{parentName:"td"},"current.mpi.cc"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"current.mpi.broadcast_file"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"current.mpi.run"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"current.mpi.exec"),". Cluster SSH config is handled automatically inside the decorator. Requires OpenSSH and an MPI implementation are installed in the Metaflow task container. It was tested against OpenMPI, which you can find a sample Dockerfile for ",(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-mpi/blob/main/examples/Dockerfile"},"here"),"."),(0,r.kt)("td",{parentName:"tr",align:null},"Forms an MPI cluster with passwordless SSH configured at task runtime. Users can submit a ",(0,r.kt)("inlineCode",{parentName:"td"},"mpi4py")," program or compile, broadcast, and submit a C program."),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://pypi.org/project/metaflow-mpi/"},(0,r.kt)("inlineCode",{parentName:"a"},"metaflow-mpi"))),(0,r.kt)("td",{parentName:"tr",align:"center"},(0,r.kt)("a",{parentName:"td",href:"https://github.com/outerbounds/metaflow-mpi/tree/main/examples/libgrape-ldbc-graph-benchmark"},"Libgrape"))))),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"Note that these decorators are not included in the ",(0,r.kt)("inlineCode",{parentName:"p"},"metaflow")," package but they are implemented as Metaflow\nExtensions. You need to install them separately in your development environment, but they will get\npackaged automatically by Metaflow, so you don't need to include them in Docker images\nor ",(0,r.kt)("inlineCode",{parentName:"p"},"@conda"),"/",(0,r.kt)("inlineCode",{parentName:"p"},"@pypi"),". Also note that the extensions are not part of ",(0,r.kt)("a",{parentName:"p",href:"/api"},"the stable Metaflow API"),", so\nthey are subject to change.")),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"When running demanding training workload, it is advisable to use ",(0,r.kt)("a",{parentName:"p",href:"/scaling/checkpoint/introduction"},"the ",(0,r.kt)("inlineCode",{parentName:"a"},"@checkpoint"),"\ndecorator")," to ensure that no progress is lost even if a\ntask hits a spurious failure.")),(0,r.kt)("h2",{id:"low-level-access"},"Low-level access"),(0,r.kt)("p",null,"Under the hood, Metaflow guarantees that you get a desired kind and number of compute nodes running\nsimultaneously, so that they are able to communicate and coordinate amongst each other."),(0,r.kt)("p",null,"You can use this compute cluster to implement any distributed computing algorithms of your own.\nTo illustrate this, consider a simple example that sets up a cluster of tasks that communicate\nwith each other over ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Message_Passing_Interface"},"MPI"),".\nTechnically, MPI is not required - you could communicate with any protocol you want - but MPI is\na popular choice."),(0,r.kt)("h3",{id:"mpi-example"},"MPI example"),(0,r.kt)("p",null,"Let\u2019s create a simple Hello World MPI program based on\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/outerbounds/metaflow-mpi/tree/main/examples/python-hello"},"this example"),".\nThe program identifies the main node (",(0,r.kt)("inlineCode",{parentName:"p"},"rank == 0"),") that sends a message to\nall workers nodes which they receive and print out. We use\n",(0,r.kt)("a",{parentName:"p",href:"https://mpi4py.readthedocs.io/en/stable/"},"mpi4py")," as a Python wrapper for the MPI protocol."),(0,r.kt)("p",null,"First, let's create an MPI script, ",(0,r.kt)("inlineCode",{parentName:"p"},"mpi_hello.py"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import mpi4py\nfrom mpi4py import MPI\n\nif __name__ == "__main__":\n\n    comm = MPI.COMM_WORLD\n    rank = comm.Get_rank()\n    size = comm.Get_size()\n\n    if rank == 0:\n        print(f"Cluster has {size} processes")\n        for i in range(1, size):\n            msg = "Main node says hi! \ud83d\udc4b"\n            comm.send(msg, dest=i)\n    else:\n        msg = comm.recv()\n        print(f"\ud83d\udc77 Worker node {rank} received message: {msg}")\n')),(0,r.kt)("p",null,"Next, let's create a flow that launches a cluster of four nodes, thanks\nto ",(0,r.kt)("inlineCode",{parentName:"p"},"num_parallel=4"),", and runs the MPI script we defined above in the cluster,\nlaunching two worker processes on each node."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from metaflow import FlowSpec, step, batch, mpi, current\n\nN_CPU = 2\nN_NODES = 4\n\nclass MPI4PyFlow(FlowSpec):\n\n    @step\n    def start(self):\n        self.next(self.multinode, num_parallel=N_NODES)\n\n    @batch(image="eddieob/mpi-base:1", cpu=N_CPU)\n    @mpi\n    @step\n    def multinode(self):\n        current.mpi.exec(\n            args=["-n", str(N_CPU * N_NODES), "--allow-run-as-root"],\n            program="python mpi_hello.py",\n        )\n        self.next(self.join)\n\n    @step\n    def join(self, inputs):\n        self.next(self.end)\n\n    @step\n    def end(self):\n        pass\n\nif __name__ == "__main__":\n    MPI4PyFlow()\n')),(0,r.kt)("p",null,"To run the flow, make sure your AWS Batch environment is\n",(0,r.kt)("a",{parentName:"p",href:"https://outerbounds.com/engineering/operations/distributed-computing/"},"configured to support multinode\njobs"),". Then, install\nthe ",(0,r.kt)("inlineCode",{parentName:"p"},"MPI")," extension for Metaflow"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"pip install metaflow-mpi\n")),(0,r.kt)("p",null,"and run the flow with"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"python mpiflow.py run\n")),(0,r.kt)("p",null,"The example uses an image, ",(0,r.kt)("inlineCode",{parentName:"p"},"eddieob/mpi-base"),", defined in\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/outerbounds/metaflow-mpi/blob/main/examples/Dockerfile"},"this Dockerfile"),". The image\nincludes MPI and ",(0,r.kt)("inlineCode",{parentName:"p"},"ssh")," for communication. Note that Metaflow packages ",(0,r.kt)("inlineCode",{parentName:"p"},"mpi_hello.py")," automatically,\nso it doesn't have to be included in the image."))}d.isMDXComponent=!0}}]);