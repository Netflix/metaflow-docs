"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[7817],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>c});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),m=p(n),c=o,f=m["".concat(l,".").concat(c)]||m[c]||u[c]||i;return n?a.createElement(f,s(s({ref:t},d),{},{components:n})):a.createElement(f,s({ref:t},d))}));function c(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,s=new Array(i);s[0]=m;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r.mdxType="string"==typeof e?e:o,s[1]=r;for(var p=2;p<i;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},1910:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>p});var a=n(7462),o=(n(7294),n(3905));const i={},s="Loading and Storing Data",r={unversionedId:"scaling/data",id:"scaling/data",title:"Loading and Storing Data",description:"This chapter describes tools and patterns for moving data in and out of your Metaflow",source:"@site/docs/scaling/data.md",sourceDirName:"scaling",slug:"/scaling/data",permalink:"/scaling/data",draft:!1,editUrl:"https://github.dev/Netflix/metaflow-docs/blob/master/docs/scaling/data.md",tags:[],version:"current",frontMatter:{},sidebar:"python",previous:{title:"Dealing with Failures",permalink:"/scaling/failures"},next:{title:"Managing External Libraries",permalink:"/scaling/dependencies"}},l={},p=[{value:"Data in Tables",id:"data-in-tables",level:2},{value:"<strong>Use cases</strong>",id:"use-cases",level:3},{value:"Data in S3: <code>metaflow.S3</code>",id:"data-in-s3-metaflows3",level:2},{value:"<strong>Pros</strong>",id:"pros",level:4},{value:"<strong>Cons</strong>",id:"cons",level:4},{value:"<strong>Use cases</strong>",id:"use-cases-1",level:3},{value:"Choosing the context",id:"choosing-the-context",level:3},{value:"<strong>Store and load objects in a Metaflow flow</strong>",id:"store-and-load-objects-in-a-metaflow-flow",level:4},{value:"<strong>Load external objects produced by a Metaflow run</strong>",id:"load-external-objects-produced-by-a-metaflow-run",level:4},{value:"<strong>Store and load objects to/from a known S3 location</strong>",id:"store-and-load-objects-tofrom-a-known-s3-location",level:4},{value:"<strong>The S3 result object</strong>",id:"the-s3-result-object",level:3},{value:"<strong>Querying objects without downloading them</strong>",id:"querying-objects-without-downloading-them",level:4},{value:"Operations on multiple objects",id:"operations-on-multiple-objects",level:3},{value:"<strong>Load multiple objects in parallel</strong>",id:"load-multiple-objects-in-parallel",level:4},{value:"<strong>Load all objects recursively under a prefix</strong>",id:"load-all-objects-recursively-under-a-prefix",level:4},{value:"Loading parts of files",id:"loading-parts-of-files",level:4},{value:"<strong>Store multiple objects or files</strong>",id:"store-multiple-objects-or-files",level:4},{value:"<strong>Listing objects in S3</strong>",id:"listing-objects-in-s3",level:4},{value:"Caution: Overwriting data in S3",id:"caution-overwriting-data-in-s3",level:3},{value:"Data in Local Files",id:"data-in-local-files",level:2}],d={toc:p};function u(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"loading-and-storing-data"},"Loading and Storing Data"),(0,o.kt)("p",null,"This chapter describes tools and patterns for moving data in and out of your Metaflow\nflows."),(0,o.kt)("p",null,"Besides the mundane concern of loading data, there is also the question of how to\norganize code related to model-specific data transformations, such as feature\nengineering. Short answer: ",(0,o.kt)("a",{parentName:"p",href:"http://en.wikipedia.org/wiki/Separation_of_concerns"},"keep data access separate from feature\nengineering"),"."),(0,o.kt)("p",null,"In a perfect world, the data scientist could design and test features without having to\nconcern themselves with the underlying mechanics of data transfer and processing.\nUnfortunately the larger the dataset, the more intermingled the two concerns become."),(0,o.kt)("p",null,"Metaflow can not make the world perfect yet. However, we recommend that data science\nworkflows try to keep the two concerns as separate as possible. In practice, you should\nuse the solutions presented in this chapter purely to load a clean dataset in your\nworkflow. Then, you should perform any model-specific data transformations in your\nPython code. In particular, we recommend that you use SQL only for data access, not for\nmodel-specific data manipulation."," "),(0,o.kt)("p",null,"There are multiple benefits in keeping data access separate from model-specific data\nmanipulation:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"It is easier to keep a model and its features in sync when they are computed together.\n",(0,o.kt)("a",{parentName:"li",href:"/scaling/tagging#tagging"},"Metaflow's built-in versioning")," makes it easy to iterate\non multiple concurrent versions of the model safely. However, Metaflow can't protect\nyou against stale input data. It is frustrating to troubleshoot bad model results that\nare caused by out-of-sync features."),(0,o.kt)("li",{parentName:"ul"},"It is quicker to iterate on your model. Testing and debugging Python is easier than\ntesting and debugging SQL."),(0,o.kt)("li",{parentName:"ul"},"You can request ",(0,o.kt)("a",{parentName:"li",href:"/scaling/remote-tasks/introduction"},"arbitrary amount of resources"),"\nfor your data manipulation needs."),(0,o.kt)("li",{parentName:"ul"},"Instead of having data manipulation code in two places (SQL and Python), all code can\nbe clearly laid out in a single place, in a single language, for maximum readability."),(0,o.kt)("li",{parentName:"ul"},"It is easier to optimize your code for performance when IO bottlenecks can be profiled\nseparately from CPU bottlenecks.")),(0,o.kt)("p",null,"Keep this guideline in mind when choosing the right data access method below."," "),(0,o.kt)("h2",{id:"data-in-tables"},"Data in Tables"),(0,o.kt)("p",null,"Accessing data in tables (most often Hive) is by far the most common way to load input\ndata to Metaflow workflows. A common paradigm is to issue arbitrary SQL queries against\nthe data warehouse to fetch data. However, depending on the data volume and the\ncomplexity of the query, queries can be slow to execute and can potentially congest the\nquery engine."),(0,o.kt)("p",null,"It is not uncommon for a data science workflow to hit these limitations. Even if your\ndata set is not huge, you may want to build multiple models in parallel, e.g. one per\ncountry. In this case, each model needs to load a shard of data. If you used SQL to load\nthe shards, it will very quickly overload your query engine."),(0,o.kt)("p",null,"As a solution, ",(0,o.kt)("a",{parentName:"p",href:"/scaling/data#data-in-s3-metaflows3"},(0,o.kt)("inlineCode",{parentName:"a"},"metaflow.S3"))," provides a way to load\ndata directly from S3, bypassing any query engines such as Spark. Combined with a\n",(0,o.kt)("a",{parentName:"p",href:"https://github.com/Netflix/metacat"},"metadata catalog"),", it is easy to write shims on top\nof ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," to directly interface with data files on S3 backing your tables. Since\ndata is loaded directly from S3, there is no limitation to the number of parallel\nprocesses. The size of data is only limited by the size of your instance, which can be\neasily controlled with ",(0,o.kt)("a",{parentName:"p",href:"/scaling/remote-tasks/introduction#requesting-resources-with-resources-decorator"},"the ",(0,o.kt)("inlineCode",{parentName:"a"},"@resources"),"\ndecorator"),".\nThe best part is that this approach is blazingly fast compared to executing SQL."),(0,o.kt)("p",null,"The main downside of this approach is that the table needs to have partitions that match\nyour access pattern. For small and medium-sized tables, this isn't necessarily an issue\nas you can afford loading extra data. Additional filtering can be performed in your\nPython code. With larger tables this approach is not feasible, so you may need to run an\nextra SQL query to repartition data properly."),(0,o.kt)("h3",{id:"use-cases"},(0,o.kt)("strong",{parentName:"h3"},"Use cases")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Workflows that need to process large amounts of data."),(0,o.kt)("li",{parentName:"ul"},"Workflows that build many models in parallel."),(0,o.kt)("li",{parentName:"ul"},"Performance-oriented workflows.")),(0,o.kt)("h2",{id:"data-in-s3-metaflows3"},"Data in S3: ",(0,o.kt)("inlineCode",{parentName:"h2"},"metaflow.S3")),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"This section contains an overview of ",(0,o.kt)("inlineCode",{parentName:"em"},"metaflow.S3"),". For a complete API, see ",(0,o.kt)("a",{parentName:"em",href:"/api/s3"},"the API\nreference for the S3 class"),".")),(0,o.kt)("p",null,"It is not always appropriate to store data in a table. For instance, Netflix has many\nsystems that communicate via JSON files in S3. Or, there is little benefit in storing a\nlarge Keras model serialized with\n",(0,o.kt)("a",{parentName:"p",href:"https://keras.io/getting-started/faq/#how-can-i-save-a-%20keras-model"},(0,o.kt)("inlineCode",{parentName:"a"},"model.save()")),"\nin a table."),(0,o.kt)("p",null,"When you assign anything to ",(0,o.kt)("inlineCode",{parentName:"p"},"self")," in your Metaflow flow, the object gets automatically\npersisted in S3 as ",(0,o.kt)("a",{parentName:"p",href:"/metaflow/basics#linear"},"a Metaflow artifact"),". Hence, in most\ncases you do not need to worry about saving data or models to S3 explicitly. We\nrecommend that you use Metaflow artifacts whenever possible, since they are easily\naccessible through ",(0,o.kt)("a",{parentName:"p",href:"/metaflow/client"},"the Client API")," by you, by other people, and by\nother workflows."),(0,o.kt)("p",null,"However, there are valid reasons for interacting with S3 directly. For instance, you may\nneed to consume or produce data to a 3rd party system that knows nothing about Metaflow.\nFor use cases like this, we provide a high-performance S3 client, ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3"),"."),(0,o.kt)("p",null,"The sole benefit of ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," over Metaflow artifacts is that you get to see and\ncontrol the S3 locations for data. Also, you must take care of object serialization by\nyourself: ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," only deals with objects of type ",(0,o.kt)("inlineCode",{parentName:"p"},"str"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"unicode"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"bytes"),"."),(0,o.kt)("p",null,"Compared to other S3 clients ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," provides two key benefits: First, when used\nin Metaflow flows, it can piggyback on Metaflow versioning, which makes it easy to track\nthe lineage of an object back to the Metaflow run that produced it. Secondly,\n",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," provides better throughput than any other S3 client that we are aware of.\nIn other words, it is very fast at loading and storing large amounts of data in S3."),(0,o.kt)("h4",{id:"pros"},(0,o.kt)("strong",{parentName:"h4"},"Pros")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Load and store data to/from arbitrary S3 locations."),(0,o.kt)("li",{parentName:"ul"},"Built-in support for lineage and versioning."),(0,o.kt)("li",{parentName:"ul"},"Maximum throughput between S3 and a compute instance.")),(0,o.kt)("h4",{id:"cons"},(0,o.kt)("strong",{parentName:"h4"},"Cons")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Don't use ",(0,o.kt)("inlineCode",{parentName:"li"},"metaflow.S3")," if you can use Metaflow artifacts instead. In contrast to\nMetaflow artifacts, ",(0,o.kt)("inlineCode",{parentName:"li"},"metaflow.S3")," is more tedious to use, uses space more wastefully,\nand it is less suitable for ",(0,o.kt)("a",{parentName:"li",href:"/scaling/data#caution-overwriting-data-in-s3"},"moving data between Metaflow steps\nreliably"),".")),(0,o.kt)("h3",{id:"use-cases-1"},(0,o.kt)("strong",{parentName:"h3"},"Use cases")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Communication with external systems through files in S3."),(0,o.kt)("li",{parentName:"ul"},"Special corner cases where you need more control over object serialization than what\nMetaflow artifacts provide by default.")),(0,o.kt)("p",null,"We recommend that you use ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," in a ",(0,o.kt)("inlineCode",{parentName:"p"},"with")," scope in Python. Objects retrieved\nfrom S3 are stored in local temporary files for the lifetime of the ",(0,o.kt)("inlineCode",{parentName:"p"},"with")," scope, not in\nmemory. You can use ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," without ",(0,o.kt)("inlineCode",{parentName:"p"},"with")," but in this case you need to call\n",(0,o.kt)("inlineCode",{parentName:"p"},"s3.close()")," to get rid of the temporary files. See examples of this below."),(0,o.kt)("p",null,"Note that in order to get the maximum performance out of ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3"),", you need to set\nyour ",(0,o.kt)("inlineCode",{parentName:"p"},"@resources")," properly. However, don't request more resources than what your\nworkload actually needs."),(0,o.kt)("h3",{id:"choosing-the-context"},"Choosing the context"),(0,o.kt)("p",null,"To benefit from the built-in support for versioning, first you need to tell\n",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," whether it is being used in the context of a Metaflow run. A run can refer\nto a currently running flow (",(0,o.kt)("inlineCode",{parentName:"p"},"run=self"),") or a past run, ",(0,o.kt)("inlineCode",{parentName:"p"},"run=Run(...)"),". If ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," is not\nspecified, ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," can be used to access data without versioning in arbitrary S3\nlocations."),(0,o.kt)("h4",{id:"store-and-load-objects-in-a-metaflow-flow"},(0,o.kt)("strong",{parentName:"h4"},"Store and load objects in a Metaflow flow")),(0,o.kt)("p",null,"We expect that the most common use case for ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," is to store auxiliary data in\na Metaflow flow. Here is an example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import FlowSpec, step, S3\nimport json\n\nclass S3DemoFlow(FlowSpec):\n\n    @step\n    def start(self):\n        with S3(run=self) as s3:\n            message = json.dumps({'message': 'hello world!'})\n            url = s3.put('example_object', message)\n            print(\"Message saved at\", url)\n        self.next(self.end)\n\n    @step\n    def end(self):\n        with S3(run=self) as s3:\n            s3obj = s3.get('example_object')\n            print(\"Object found at\", s3obj.url)\n            print(\"Message:\", json.loads(s3obj.text))\n\nif __name__ == '__main__':\n    S3DemoFlow()\n")),(0,o.kt)("p",null,"Running the flow produced the following output:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"Workflow starting (run-id 3):\n[3/start/646436 (pid 30559)] Task is starting.\n[3/start/646436 (pid 30559)] Message saved at s3://my-bucket/metaflow/userdata/v1/S3DemoFlow/3/example_object\n[3/start/646436 (pid 30559)] Task finished successfully.\n[3/end/646437 (pid 30619)] Task is starting.\n[3/end/646437 (pid 30619)] Object found at s3://my-bucket/metaflow/userdata/v1/S3DemoFlow/3/example_object\n[3/end/646437 (pid 30619)] Message: {'message': 'hello world!'}\n[3/end/646437 (pid 30619)] Task finished successfully.\n")),(0,o.kt)("p",null,"Now you could share the URL,\n",(0,o.kt)("inlineCode",{parentName:"p"},"s3://my-bucket/metaflow/userdata/v1/S3DemoFlow/3/example_object"),", with external\nsystems. Note that the URL includes both the flow name, ",(0,o.kt)("inlineCode",{parentName:"p"},"S3DemoFlow"),", as well as its\nunique run id, ",(0,o.kt)("inlineCode",{parentName:"p"},"3"),", which allow us to track the lineage of the object back to the run\nthat produced it."),(0,o.kt)("p",null,"Note that ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," provides a default S3 location for storing data. You could\nchange the location by defining ",(0,o.kt)("inlineCode",{parentName:"p"},"S3(bucket='my-bucket', prefix='/my/prefix')")," for the\nconstructor. Metaflow versioning information would be concatenated to the ",(0,o.kt)("inlineCode",{parentName:"p"},"prefix"),"."),(0,o.kt)("h4",{id:"load-external-objects-produced-by-a-metaflow-run"},(0,o.kt)("strong",{parentName:"h4"},"Load external objects produced by a Metaflow run")),(0,o.kt)("p",null,"What if you want to inspect S3 data produced by a flow afterwards? Just use ",(0,o.kt)("a",{parentName:"p",href:"/metaflow/client"},"the Client\nAPI")," as usual to locate the desired ",(0,o.kt)("inlineCode",{parentName:"p"},"Run")," and use it to initialize\nan ",(0,o.kt)("inlineCode",{parentName:"p"},"S3")," object:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith S3(run=Flow('S3DemoFlow').latest_run) as s3:\n    print(s3.get('example_object').text)\n\n{\"message\": \"hello world!\"}\n")),(0,o.kt)("p",null,"This pattern is particularly convenient for notebooks."),(0,o.kt)("h4",{id:"store-and-load-objects-tofrom-a-known-s3-location"},(0,o.kt)("strong",{parentName:"h4"},"Store and load objects to/from a known S3 location")),(0,o.kt)("p",null,"The above examples inferred the S3 location based on the current or an existing Metaflow\nrun. What if you want to load data that has nothing to do with Metaflow? Easy:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith S3() as s3:\n    res = s3.get('s3://my-bucket/savin/tmp/external_data')\n    print('an alien message: %s' % res.text)\n\nan alien message: I know nothing about Metaflow\n")),(0,o.kt)("p",null,"If ",(0,o.kt)("inlineCode",{parentName:"p"},"S3")," is initialized without any arguments, all operations require a full S3 URL."),(0,o.kt)("p",null,"If you need to operate on multiple files, it may be more convenient to specify a custom\nS3 prefix with the ",(0,o.kt)("inlineCode",{parentName:"p"},"s3root")," argument:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith S3(s3root='s3://my-bucket/savin/tmp/s3demo/') as s3:\n    s3.put('fruit', 'pineapple')\n    s3.put('animal', 'mongoose')\nwith S3() as s3:\n    s3.get('s3://my-bucket/savin/tmp/s3demo/fruit').text\n\npineapple\n")),(0,o.kt)("p",null,"If the requested URL does not exist, the ",(0,o.kt)("inlineCode",{parentName:"p"},"get")," call will raise an exception. You can\ncall ",(0,o.kt)("inlineCode",{parentName:"p"},"get")," with ",(0,o.kt)("inlineCode",{parentName:"p"},"return_missing=True")," if you want to return a missing URL as an ordinary\nresult object, as described in the section below."),(0,o.kt)("p",null,"By default, ",(0,o.kt)("inlineCode",{parentName:"p"},"put_*")," calls will overwrite existing keys in S3. To avoid this behavior you\ncan invoke your ",(0,o.kt)("inlineCode",{parentName:"p"},"put_*")," calls with ",(0,o.kt)("inlineCode",{parentName:"p"},"overwrite=False"),". Refer to ",(0,o.kt)("a",{parentName:"p",href:"/scaling/data#caution-overwriting-data-in-s3"},"this\nsection")," for some of the pitfalls involved with\noverwriting keys in S3."),(0,o.kt)("h3",{id:"the-s3-result-object"},(0,o.kt)("strong",{parentName:"h3"},"The S3 result object")),(0,o.kt)("p",null,"All ",(0,o.kt)("inlineCode",{parentName:"p"},"get")," operations return an ",(0,o.kt)("inlineCode",{parentName:"p"},"S3Object"),", backed by a temporary file on local disk,\nwhich exposes a number of attributes about the object:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"with S3(s3root='s3://my-bucket/savin/tmp/s3demo/') as s3:\n    s3obj = s3.get('fruit')\n    print('location', s3obj.url)\n    print('key', s3obj.key)\n    print('size', s3obj.size)\n    print('local path', s3obj.path)\n    print('bytes', s3obj.blob)\n    print('unicode', s3obj.text)\n    print('metadata', s3obj.metadata)\n    print('content-type', s3obj.content_type)\n    print('downloaded', s3obj.downloaded)\n\nlocation s3://my-bucket/savin/tmp/s3demo/fruit\nkey fruit\nsize 9\nlocal path /data/metaflow/metaflow.s3.5agi129m/metaflow.s3.one_file.pih_iseg\nbytes b'pineapple'\nunicode pineapple\nmetadata None\ncontent-type application/octet-stream\ndownloaded True\n")),(0,o.kt)("p",null,"Note that you can not access data behind ",(0,o.kt)("inlineCode",{parentName:"p"},"s3obj")," outside the ",(0,o.kt)("inlineCode",{parentName:"p"},"with")," scope as the\ntemporary file pointed at ",(0,o.kt)("inlineCode",{parentName:"p"},"s3obj.path")," will get deleted as the scope exits."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"S3Object")," may also refer to an S3 URL that does not correspond to an object in S3.\nThese objects have ",(0,o.kt)("inlineCode",{parentName:"p"},"exists")," property set to ",(0,o.kt)("inlineCode",{parentName:"p"},"False"),". Non-existent objects may be\nreturned by a ",(0,o.kt)("inlineCode",{parentName:"p"},"list_path")," call, if the result refers to an S3 prefix, not an object.\nListing operations also set ",(0,o.kt)("inlineCode",{parentName:"p"},"downloaded")," property to ",(0,o.kt)("inlineCode",{parentName:"p"},"False"),", to distinguish them from\noperations that download data locally. Also ",(0,o.kt)("inlineCode",{parentName:"p"},"get")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many")," may return non-existent\nobjects if you call these methods with an argument ",(0,o.kt)("inlineCode",{parentName:"p"},"return_missing=True"),"."),(0,o.kt)("h4",{id:"querying-objects-without-downloading-them"},(0,o.kt)("strong",{parentName:"h4"},"Querying objects without downloading them")),(0,o.kt)("p",null,"The above information about an object, like ",(0,o.kt)("inlineCode",{parentName:"p"},"size")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"metadata"),", can be useful even\nwithout downloading the file itself. To just get the metadata, use the ",(0,o.kt)("inlineCode",{parentName:"p"},"info")," and\n",(0,o.kt)("inlineCode",{parentName:"p"},"info_many")," calls that work like ",(0,o.kt)("inlineCode",{parentName:"p"},"get")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many")," but avoid the potentially\nexpensive downloading part. The info calls set ",(0,o.kt)("inlineCode",{parentName:"p"},"downloaded=False")," in the result object."),(0,o.kt)("h3",{id:"operations-on-multiple-objects"},"Operations on multiple objects"),(0,o.kt)("p",null,"After you have instantiated the object given the right context information, all ",(0,o.kt)("inlineCode",{parentName:"p"},"get"),"\nand ",(0,o.kt)("inlineCode",{parentName:"p"},"put")," operations work equally. The context is only used to construct an appropriate\nS3 URL."),(0,o.kt)("p",null,"Besides loading individual files with ",(0,o.kt)("inlineCode",{parentName:"p"},".get()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},".put()")," as shown above,\n",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," really shines at operating multiple files at once."),(0,o.kt)("p",null,"It is guaranteed that the list of ",(0,o.kt)("inlineCode",{parentName:"p"},"S3Objects")," returned is always in the same order as\nlong as the underlying data does not change. This can be important e.g. if you use\n",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," to feed data for a model. The input data will be in a deterministic order\nso results should be easily reproducible."),(0,o.kt)("h4",{id:"load-multiple-objects-in-parallel"},(0,o.kt)("strong",{parentName:"h4"},"Load multiple objects in parallel")),(0,o.kt)("p",null,"Use ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many()")," to load arbitrarily many objects at once:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith S3(s3root='s3://my-bucket/savin/tmp/s3demo/') as s3:\n    s3.get_many(['fruit', 'animal'])\n\n[<S3Object s3://my-bucket/savin/tmp/s3demo/fruit (9 bytes)>,\n <S3Object s3://my-bucket/savin/tmp/s3demo/animal (8 bytes)>]\n")),(0,o.kt)("p",null,"Here, ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many()")," loads objects in parallel, which is much faster than loading\nindividual objects sequentially. You can achieve the optimal throughput with S3 only\nwhen you operate on many files in parallel."),(0,o.kt)("p",null,"If one of the requested URLs doesn't exist, the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many")," call will raise an exception.\nIf you don't want to fail all objects because of missing URLs, call ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many")," with\n",(0,o.kt)("inlineCode",{parentName:"p"},"return_missing=True"),". This will make ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many")," return missing URLs amongst other\nresults. You can distinguish between the found and not found URLs using the ",(0,o.kt)("inlineCode",{parentName:"p"},"exists"),"\nproperty of ",(0,o.kt)("inlineCode",{parentName:"p"},"S3Object"),"."),(0,o.kt)("h4",{id:"load-all-objects-recursively-under-a-prefix"},(0,o.kt)("strong",{parentName:"h4"},"Load all objects recursively under a prefix")),(0,o.kt)("p",null,"We can load all objects under a given prefix:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith S3() as s3:\n    s3.get_recursive(['s3://my-bucket/savin/tmp/s3demo'])\n\n[<S3Object s3://my-bucket/savin/tmp/s3demo/animal (8 bytes)>,\n <S3Object s3://my-bucket/savin/tmp/s3demo/fruit (9 bytes)>]\n")),(0,o.kt)("p",null,"Note that ",(0,o.kt)("inlineCode",{parentName:"p"},"get_recursive")," takes a list of prefixes. This is useful for achieving the\nmaximum level of parallelism when retrieving data under multiple prefixes."),(0,o.kt)("p",null,"If you have specified a custom ",(0,o.kt)("inlineCode",{parentName:"p"},"s3root"),", you can use ",(0,o.kt)("inlineCode",{parentName:"p"},"get_all()")," to get all files\nrecursively under the given prefix."),(0,o.kt)("h4",{id:"loading-parts-of-files"},"Loading parts of files"),(0,o.kt)("p",null,"A performance-sensitive application may want to read only a part of a large file.\nInstead of a string, the ",(0,o.kt)("inlineCode",{parentName:"p"},"get")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many")," calls also accept an object with ",(0,o.kt)("inlineCode",{parentName:"p"},"key"),",\n",(0,o.kt)("inlineCode",{parentName:"p"},"offset"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"length")," attributes that specify a part of a file to download. You can use an\nobject called ",(0,o.kt)("inlineCode",{parentName:"p"},"S3GetObject")," provided by Metaflow for this purpose."),(0,o.kt)("p",null,"This example loads two 1KB chunks of a file in S3:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nfrom metaflow.datatools.s3 import S3GetObject\n\nURL = 's3://ursa-labs-taxi-data/2014/12/data.parquet'\n\nwith S3() as s3:\n    res = s3.get_many([S3GetObject(key=URL, offset=0, length=1024),\n                       S3GetObject(key=URL, offset=1024, length=1024)])\n\n    for obj in res:\n        print(obj.path, obj.size)\n")),(0,o.kt)("h4",{id:"store-multiple-objects-or-files"},(0,o.kt)("strong",{parentName:"h4"},"Store multiple objects or files")),(0,o.kt)("p",null,"If you need to store multiple objects, use ",(0,o.kt)("inlineCode",{parentName:"p"},"put_many"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nmany = {'first_key': 'foo', 'second_key': 'bar'}\nwith S3(s3root='s3://my-bucket/savin/tmp/s3demo_put/') as s3:\n    s3.put_many(many.items())\n\n[('first_key', 's3://my-bucket/savin/tmp/s3demo_put/first_key'),\n ('second_key', 's3://my-bucket/savin/tmp/s3demo_put/second_key')]\n")),(0,o.kt)("p",null,"You may want to store more data to S3 than what you can fit in memory at once. This is a\ngood use case for ",(0,o.kt)("inlineCode",{parentName:"p"},"put_files"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith open('/tmp/1', 'w') as f:\n    f.write('first datum')\nwith open('/tmp/2', 'w') as f:\n    f.write('second datum')\nwith S3(s3root='s3://my-bucket/savin/tmp/s3demo_put/') as s3:\n    s3.put_files([('first_file', '/tmp/1'), ('second_file', '/tmp/2')])\n\n[('first_file', 's3://my-bucket/savin/tmp/s3demo_put/first_file'),\n ('second_file', 's3://my-bucket/savin/tmp/s3demo_put/second_file')]\n")),(0,o.kt)("p",null,"Objects are stored in S3 in parallel for maximum throughput."),(0,o.kt)("h4",{id:"listing-objects-in-s3"},(0,o.kt)("strong",{parentName:"h4"},"Listing objects in S3")),(0,o.kt)("p",null,"To get objects with ",(0,o.kt)("inlineCode",{parentName:"p"},"get")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many"),", you need to know the exact names of the\nobjects to download. S3 is optimized for looking up specific names, so it is preferable\nto structure your code around known names. However, sometimes this is not possible and\nyou need to check first what is available in S3."),(0,o.kt)("p",null,"Metaflow provides two ways to list objects in S3: ",(0,o.kt)("inlineCode",{parentName:"p"},"list_paths")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"list_recursive"),". The\nfirst method provides the next level of prefixes (directories) in S3, directly under the\ngiven prefix. The latter method provides all objects under the given prefix. Since\n",(0,o.kt)("inlineCode",{parentName:"p"},"list_paths")," returns a subset of prefixes returned by ",(0,o.kt)("inlineCode",{parentName:"p"},"list_recursive"),", it is typically\na much faster operation."),(0,o.kt)("p",null,"Here's an example: First, let's create files in S3 in a hierarchy like this:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"first/a/object1\nfirst/b/x/object2\nsecond/c/object3\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nmany = {'first/a/object1': 'data',\n        'first/b/x/object2': 'data',\n        'second/c/object3': 'data'}\nwith S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:\n    s3.put_many(many.items())\n")),(0,o.kt)("p",null,"Next, let's list all directories using ",(0,o.kt)("inlineCode",{parentName:"p"},"list_paths"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:\n    for key in s3.list_paths():\n        print key.key\n\nfirst\nsecond\n")),(0,o.kt)("p",null,"You can list multiple prefixes in parallel by giving ",(0,o.kt)("inlineCode",{parentName:"p"},"list_paths")," a list of prefixes:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:\n    for key in s3.list_paths(['first', 'second']):\n        print key.key\n\na\nb\nc\n")),(0,o.kt)("p",null,"Listing may return either prefixes (directories) or objects. To distinguish between the\ntwo, use the ",(0,o.kt)("inlineCode",{parentName:"p"},".exists")," property of the returned ",(0,o.kt)("inlineCode",{parentName:"p"},"S3Object"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:\n    for key in s3.list_paths(['first/a', 'first/b']):\n        print key.key, 'object' if key.exists else 'prefix'\n\nobject1 object\nx prefix\n")),(0,o.kt)("p",null,"If you want all objects under the given prefix, use the ",(0,o.kt)("inlineCode",{parentName:"p"},"list_recursive")," method:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import S3\nwith S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:\n    for key in s3.list_recursive():\n        print key.key\n\nfirst/a/object1\nfirst/b/x/object2\nsecond/c/object3\n")),(0,o.kt)("p",null,"Similar to ",(0,o.kt)("inlineCode",{parentName:"p"},"list_paths"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"list_recursive")," can take a list of prefixes to process in\nparallel."),(0,o.kt)("p",null,"A common pattern is to list objects using either ",(0,o.kt)("inlineCode",{parentName:"p"},"list_paths")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"list_recursive"),",\nfilter out some keys from the listing, and provide the pruned list to ",(0,o.kt)("inlineCode",{parentName:"p"},"get_many")," for\nfast parallelized downloading."),(0,o.kt)("h3",{id:"caution-overwriting-data-in-s3"},"Caution: Overwriting data in S3"),(0,o.kt)("p",null,"You should avoid overwriting data in the same key (URL) in S3. S3 guarantees that new\nkeys always reflect the latest data. In contrast, when you overwrite data in an existing\nkey, there is a short period of time when a reader may see either the old version or the\nnew version of the data."),(0,o.kt)("p",null,"In particular, when you use ",(0,o.kt)("inlineCode",{parentName:"p"},"metaflow.S3")," in your Metaflow flows, make sure that every\ntask and step writes to a unique key. Otherwise you may find results unpredictable and\ninconsistent."),(0,o.kt)("p",null,"Note that specifying ",(0,o.kt)("inlineCode",{parentName:"p"},"overwrite=False")," in your ",(0,o.kt)("inlineCode",{parentName:"p"},"put_*")," calls changes the behavior of S3\nslightly compared to the default mode of ",(0,o.kt)("inlineCode",{parentName:"p"},"overwrite=True"),". There may be a small delay\n(typically in the order of milliseconds) before the key becomes available for reading."),(0,o.kt)("p",null,"This is an important reason to rely on Metaflow artifacts, which handle this\ncomplication for you, whenever possible. If you absolutely need to handle this by\nyourself, one way to guarantee uniqueness is to use ",(0,o.kt)("inlineCode",{parentName:"p"},"current.task_id")," from ",(0,o.kt)("a",{parentName:"p",href:"/scaling/tagging#accessing-current-ids-in-a-flow"},"the\n",(0,o.kt)("inlineCode",{parentName:"a"},"current")," module")," as a part of your\nS3 keys."),(0,o.kt)("h2",{id:"data-in-local-files"},"Data in Local Files"),(0,o.kt)("p",null,"Similarly to ",(0,o.kt)("a",{parentName:"p",href:"/metaflow/basics#how-to-define-parameters-for-flows"},"Parameters"),", you\ncan define a data file to include as input for your flow. Metaflow will version the file\nand make it accessible to all the steps directly through the ",(0,o.kt)("inlineCode",{parentName:"p"},"self")," object in your flow."),(0,o.kt)("p",null,"This example allows the user to include a data file and compute its hash:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import FlowSpec, step, IncludeFile\n\nclass HashFileFlow(FlowSpec):\n    myfile = IncludeFile(\n        'myfile',\n        is_text=False,\n        help='My input',\n        default='/Users/bob/myinput.bin')\n\n    @step\n    def start(self):\n        import hashlib\n        print('Hello from start')\n        print('Hash of file is %s' % \\\n            str(hashlib.sha1(self.myfile).hexdigest()))\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print('Goodbye')\n\nif __name__ == '__main__':\n    HashFileFlow()\n")),(0,o.kt)("p",null,"You can specify the file to use by using:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python hash_flow.py run --myfile '/path/to/input/file'\n")))}u.isMDXComponent=!0}}]);