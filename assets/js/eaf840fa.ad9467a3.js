"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[600],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>u});var o=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,i=function(e,t){if(null==e)return{};var n,o,i={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var c=o.createContext({}),s=function(e){var t=o.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},p=function(e){var t=s(e.components);return o.createElement(c.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,c=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),h=s(n),u=i,m=h["".concat(c,".").concat(u)]||h[u]||d[u]||r;return n?o.createElement(m,a(a({ref:t},p),{},{components:n})):o.createElement(m,a({ref:t},p))}));function u(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,a=new Array(r);a[0]=h;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:i,a[1]=l;for(var s=2;s<r;s++)a[s]=n[s];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}h.displayName="MDXCreateElement"},2581:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>s});var o=n(7462),i=(n(7294),n(3905));const r={},a="Checkpoints in ML/AI libraries",l={unversionedId:"scaling/checkpoint/checkpoint-ml-libraries",id:"scaling/checkpoint/checkpoint-ml-libraries",title:"Checkpoints in ML/AI libraries",description:"Let's explore how @checkpoint works in a real-world scenario when checkpointing training progress with popular ML",source:"@site/docs/scaling/checkpoint/checkpoint-ml-libraries.md",sourceDirName:"scaling/checkpoint",slug:"/scaling/checkpoint/checkpoint-ml-libraries",permalink:"/scaling/checkpoint/checkpoint-ml-libraries",draft:!1,editUrl:"https://github.dev/Netflix/metaflow-docs/blob/master/docs/scaling/checkpoint/checkpoint-ml-libraries.md",tags:[],version:"current",frontMatter:{},sidebar:"python",previous:{title:"Checkpointing Progress",permalink:"/scaling/checkpoint/introduction"},next:{title:"Selecting a Checkpoint to Use",permalink:"/scaling/checkpoint/selecting-checkpoints"}},c={},s=[{value:"Checkpointing XGBoost",id:"checkpointing-xgboost",level:2},{value:"Checkpointing PyTorch",id:"checkpointing-pytorch",level:2},{value:"Checkpointing GenAI/LLM fine-tuning",id:"checkpointing-genaillm-fine-tuning",level:2},{value:"Checkpointing distributed workloads",id:"checkpointing-distributed-workloads",level:2}],p={toc:s};function d(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,o.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"checkpoints-in-mlai-libraries"},"Checkpoints in ML/AI libraries"),(0,i.kt)("p",null,"Let's explore how ",(0,i.kt)("inlineCode",{parentName:"p"},"@checkpoint")," works in a real-world scenario when checkpointing training progress with popular ML\nlibraries."),(0,i.kt)("h2",{id:"checkpointing-xgboost"},"Checkpointing XGBoost"),(0,i.kt)("p",null,"Like many other ML libraries, ",(0,i.kt)("a",{parentName:"p",href:"https://xgboost.readthedocs.io/en/stable/"},"XGBoost")," allows you to define custom callbacks\nthat are called periodically during training. We can create a custom checkpointer that saves the model to a file, using\n",(0,i.kt)("inlineCode",{parentName:"p"},"pickle"),", ",(0,i.kt)("a",{parentName:"p",href:"https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html"},"as recommended by XGBoost"),", and calls\n",(0,i.kt)("inlineCode",{parentName:"p"},"current.checkpoint.save()")," to persist it."),(0,i.kt)("p",null,"Save this snippet in a file, ",(0,i.kt)("inlineCode",{parentName:"p"},"xgboost_checkpointer.py"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport pickle\nfrom metaflow import current\nimport xgboost\n\nclass Checkpointer(xgboost.callback.TrainingCallback):\n\n    @classmethod\n    def _path(cls):\n        return os.path.join(current.checkpoint.directory, 'xgb_cp.pkl')\n\n    def __init__(self, interval=10):\n        self._interval = interval\n\n    def after_iteration(self, model, epoch, evals_log):\n        if epoch > 0 and epoch % self._interval == 0:\n            with open(self._path(), 'wb') as f:\n                pickle.dump(model, f)\n            current.checkpoint.save()\n\n    @classmethod\n    def load(cls):\n        with open(cls._path(), 'rb') as f:\n            return pickle.load(f)  \n")),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"Make sure that the checkpoint directory doesn't accumulate files across invocations, which would make the ",(0,i.kt)("inlineCode",{parentName:"p"},"save"),"\noperation become slower over time. Either overwrite the same files or clean up the directory between checkpoints.\nThe ",(0,i.kt)("inlineCode",{parentName:"p"},"save")," call will create a uniquely named checkpoint directory automatically, so you can keep overwriting files\nacross iterations.")),(0,i.kt)("p",null,"We can then train an XGboost model using ",(0,i.kt)("inlineCode",{parentName:"p"},"Checkpointer"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from metaflow import FlowSpec, step, current, Flow,\\\n                     Parameter, conda, retry, checkpoint, card, timeout\n\nclass CheckpointXGBoost(FlowSpec):\n    rounds = Parameter("rounds", help="number of boosting rounds", default=128)\n\n    @conda(packages={"scikit-learn": "1.6.1"})\n    @step\n    def start(self):\n        from sklearn.datasets import load_breast_cancer\n\n        self.X, self.y = load_breast_cancer(return_X_y=True)\n        self.next(self.train)\n\n    @timeout(seconds=15)\n    @conda(packages={"xgboost": "2.1.4"})\n    @card\n    @retry\n    @checkpoint\n    @step\n    def train(self):\n        import xgboost as xgb\n        from xgboost_checkpointer import Checkpointer\n\n        if current.checkpoint.is_loaded:\n            cp_model = Checkpointer.load()\n            cp_rounds = cp_model.num_boosted_rounds()\n            print(f"Checkpoint was trained for {cp_rounds} rounds")\n        else:\n            cp_model = None\n            cp_rounds = 0\n\n        model = xgb.XGBClassifier(\n            n_estimators=self.rounds - cp_rounds,\n            eval_metric="logloss",\n            callbacks=[Checkpointer()])\n        model.fit(self.X, self.y, eval_set=[(self.X, self.y)], xgb_model=cp_model)\n\n        assert model.get_booster().num_boosted_rounds() == self.rounds\n        print("Training completed!")\n        self.next(self.end)\n\n    @step\n    def end(self):\n        pass\n\nif __name__ == "__main__":\n    CheckpointXGBoost()\n')),(0,i.kt)("p",null,"You can run the flow, saved to ",(0,i.kt)("inlineCode",{parentName:"p"},"xgboostflow.py"),", as usual:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"python xgboostflow.py --environment=conda run\n")),(0,i.kt)("p",null,"To demonstrate checkpoints in action, ",(0,i.kt)("a",{parentName:"p",href:"/scaling/failures#timing-out-with-the-timeout-decorator"},"the ",(0,i.kt)("inlineCode",{parentName:"a"},"@timeout"),"\ndecorator")," interrupts training every 15 seconds.\nYou can adjust the time\ndepending on how fast the training progresses on your workstation. The ",(0,i.kt)("inlineCode",{parentName:"p"},"@retry")," decorator will then start the task\nagain, allowing ",(0,i.kt)("inlineCode",{parentName:"p"},"@checkpoint")," to load the latest checkpoint and resume training."),(0,i.kt)("h2",{id:"checkpointing-pytorch"},"Checkpointing PyTorch"),(0,i.kt)("p",null,"Using ",(0,i.kt)("inlineCode",{parentName:"p"},"@checkpoint")," with ",(0,i.kt)("a",{parentName:"p",href:"https://pytorch.org/"},"PyTorch")," is straightforward. Within your training loop, periodically\nserialize the model and use ",(0,i.kt)("inlineCode",{parentName:"p"},"current.checkpoint.save()")," to create a checkpoint, along these lines:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"model_path = os.path.join(current.checkpoint.directory, 'model')\ntorch.save(model.state_dict(), model_path)\ncurrent.checkpoint.save()\n")),(0,i.kt)("p",null,"Before starting training, check for an available checkpoint and load the model from it if found:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"if current.checkpoint.is_loaded:\n    model.load_state_dict(torch.load(model_path))\n")),(0,i.kt)("p",null,"Take a look at ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/outerbounds/metaflow-checkpoint-examples/tree/master/mnist_torch_vanilla"},"this reference repository for a complete\nexample")," showing this pattern in action, in addition to examples for many other frameworks."),(0,i.kt)("h2",{id:"checkpointing-genaillm-fine-tuning"},"Checkpointing GenAI/LLM fine-tuning"),(0,i.kt)("p",null,"Fine-tuning large language models and other large foundation models for generative AI can easily take hours, running on expensive GPU instances. Take a look at the following examples to learn how ",(0,i.kt)("inlineCode",{parentName:"p"},"@checkpoint")," can be applied to various fine-tuning use cases:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"https://github.com/outerbounds/metaflow-checkpoint-examples/tree/master/lora_huggingface"},"Finetuning a LoRA from a model downloaded from\nHuggingFace"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"https://github.com/outerbounds/metaflow-checkpoint-examples/tree/master/llama_factory"},"Finetuning an LLM using LLaMA\nFactory"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"https://github.com/outerbounds/metaflow-checkpoint-examples/tree/master/nim_lora"},"Finetuning an LLM and serve it with NVIDIA\nNIM")))),(0,i.kt)("h2",{id:"checkpointing-distributed-workloads"},"Checkpointing distributed workloads"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"/scaling/remote-tasks/distributed-computing"},"Metaflow supports distributed training")," and other distributed workloads\nwhich execute across multiple instances in a cluster. When training large models over extended periods across multiple\ninstances, which greatly increases the likelihood of hitting spurious failures, checkpointing becomes essential to\nensure efficient recovery."),(0,i.kt)("p",null,"Checkpointing works smoothly when only the control node in a training cluster is designated to handle it, preventing\nrace conditions that could arise from multiple instances attempting to save progress simultaneously. For reference,\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/outerbounds/metaflow-checkpoint-examples/tree/master/cifar_distributed"},"take a look at this\nexample")," that uses ",(0,i.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"},"PyTorch Data Distributed Parallel (DDP)")," mode to train a vision model on CIFAR-10 dataset, checkpointing progress with ",(0,i.kt)("inlineCode",{parentName:"p"},"@checkpoint"),"."),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"Large-scale distributed computing can be challenging. If you need help setting up ",(0,i.kt)("inlineCode",{parentName:"p"},"@checkpoint")," in distributed setups, don\u2019t hesitate to ",(0,i.kt)("a",{parentName:"p",href:"http://slack.outerbounds.co"},"ask for guidance on Metaflow Slack"),".")))}d.isMDXComponent=!0}}]);