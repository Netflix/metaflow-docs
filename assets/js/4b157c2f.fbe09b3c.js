"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[8072],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>f});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,m=o(e,["components","mdxType","originalType","parentName"]),c=p(n),f=r,d=c["".concat(s,".").concat(f)]||c[f]||u[f]||l;return n?a.createElement(d,i(i({ref:t},m),{},{components:n})):a.createElement(d,i({ref:t},m))}));function f(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=n.length,i=new Array(l);i[0]=c;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:r,i[1]=o;for(var p=2;p<l;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},9220:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const l={},i="Using Multiple CPU Cores",o={unversionedId:"scaling/remote-tasks/multicore",id:"scaling/remote-tasks/multicore",title:"Using Multiple CPU Cores",description:"When running locally, tasks are executed as separate processes. The operating system",source:"@site/docs/scaling/remote-tasks/multicore.md",sourceDirName:"scaling/remote-tasks",slug:"/scaling/remote-tasks/multicore",permalink:"/scaling/remote-tasks/multicore",draft:!1,editUrl:"https://github.dev/Netflix/metaflow-docs/blob/master/docs/scaling/remote-tasks/multicore.md",tags:[],version:"current",frontMatter:{},sidebar:"python",previous:{title:"Requesting Compute Resources",permalink:"/scaling/remote-tasks/requesting-resources"},next:{title:"Using Spot Instances",permalink:"/scaling/remote-tasks/spot-instances"}},s={},p=[{value:"Mapping items in parallel",id:"mapping-items-in-parallel",level:2}],m={toc:p};function u(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"using-multiple-cpu-cores"},"Using Multiple CPU Cores"),(0,r.kt)("p",null,"When running locally, tasks are executed as separate processes. The operating system\ntakes care of allocating them to separate CPU cores, so they will actually execute in\nparallel assuming that enough CPU cores are available. Hence, your flow can utilize\nmultiple cores without you having to do anything special besides defining branches in\nthe flow."),(0,r.kt)("p",null,"When running remotely on ",(0,r.kt)("inlineCode",{parentName:"p"},"@batch")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"@kubernetes"),", branches are mapped to separate jobs\nthat are executed in parallel, allowing you to ",(0,r.kt)("em",{parentName:"p"},"scale horizontally")," to any number of\nparallel tasks. In addition, you may take advantage of multiple CPU cores inside a task.\nThis may happen automatically if you use a modern ML library like PyTorch or Scikit\nLearn, or you may parallelize functions explicitly, as explained below."),(0,r.kt)("h2",{id:"mapping-items-in-parallel"},"Mapping items in parallel"),(0,r.kt)("p",null,"Metaflow provides a utility function called ",(0,r.kt)("inlineCode",{parentName:"p"},"parallel_map")," that helps take advantage of\nmultiple CPU cores. This function is almost equivalent to ",(0,r.kt)("inlineCode",{parentName:"p"},"Pool().map")," in the Python's\nbuilt-in\n",(0,r.kt)("a",{parentName:"p",href:"https://docs.python.org/2/library/multiprocessing.html#multiprocessing.pool.multiprocessing.Pool.map"},"multiprocessing"),"\nlibrary. The main differences are the following:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"parallel_map")," supports lambdas and any other callables of Python."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"parallel_map")," does not suffer from bugs present in ",(0,r.kt)("inlineCode",{parentName:"li"},"multiprocessing"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"parallel_map")," can handle larger amounts of data.")),(0,r.kt)("p",null,"You may also use ",(0,r.kt)("inlineCode",{parentName:"p"},"parallel_map")," to parallelize simple operations that might be too\ncumbersome to implement as separate steps."),(0,r.kt)("p",null,"Here is an extension of our previous example that implements a multicore ",(0,r.kt)("inlineCode",{parentName:"p"},"sum()")," by\npartitioning the matrix by row:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from metaflow import FlowSpec, step, batch, parallel_map\n\nclass BigSum(FlowSpec):\n\n    @resources(memory=60000, cpu=8)\n    @step\n    def start(self):\n        import numpy\n        import time\n        big_matrix = numpy.random.ranf((80000, 80000))\n        t = time.time()\n        parts = parallel_map(lambda i: big_matrix[i:i+10000].sum(),\n                             range(0, 80000, 10000))\n        self.sum = sum(parts)\n        self.took = time.time() - t\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print("The sum is %f." % self.sum)\n        print("Computing it took %dms." % (self.took * 1000))\n\nif __name__ == \'__main__\':\n    BigSum()\n')),(0,r.kt)("p",null,"Note that we use ",(0,r.kt)("inlineCode",{parentName:"p"},"cpu=8")," to request enough CPU cores, so our ",(0,r.kt)("inlineCode",{parentName:"p"},"parallel_map")," can benefit\nfrom optimal parallelism. Disappointingly, in this case the parallel ",(0,r.kt)("inlineCode",{parentName:"p"},"sum")," is not faster\nthan the original simple implementation due to the overhead of launching separate\nprocesses in ",(0,r.kt)("inlineCode",{parentName:"p"},"parallel_map"),". A less trivial operation might see a much larger\nperformance boost."))}u.isMDXComponent=!0}}]);