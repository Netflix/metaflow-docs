"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[5458],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=o.createContext({}),u=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=u(e.components);return o.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},f=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),f=u(n),d=a,h=f["".concat(s,".").concat(d)]||f[d]||c[d]||r;return n?o.createElement(h,i(i({ref:t},p),{},{components:n})):o.createElement(h,i({ref:t},p))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=f;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,i[1]=l;for(var u=2;u<r;u++)i[u]=n[u];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}f.displayName="MDXCreateElement"},1032:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>u});var o=n(7462),a=(n(7294),n(3905));const r={},i="Scheduling Metaflow Flows with Apache Airflow",l={unversionedId:"production/scheduling-metaflow-flows/scheduling-with-airflow",id:"production/scheduling-metaflow-flows/scheduling-with-airflow",title:"Scheduling Metaflow Flows with Apache Airflow",description:"Apache Airflow is a popular open-source workflow",source:"@site/docs/production/scheduling-metaflow-flows/scheduling-with-airflow.md",sourceDirName:"production/scheduling-metaflow-flows",slug:"/production/scheduling-metaflow-flows/scheduling-with-airflow",permalink:"/production/scheduling-metaflow-flows/scheduling-with-airflow",draft:!1,editUrl:"https://github.dev/Netflix/metaflow-docs/blob/master/docs/production/scheduling-metaflow-flows/scheduling-with-airflow.md",tags:[],version:"current",frontMatter:{},sidebar:"python",previous:{title:"Scheduling Metaflow Flows with AWS Step Functions",permalink:"/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions"},next:{title:"Coordinating Larger Metaflow Projects",permalink:"/production/coordinating-larger-metaflow-projects"}},s={},u=[{value:"Pushing a flow to production",id:"pushing-a-flow-to-production",level:2},{value:"Hardening deployments",id:"hardening-deployments",level:3},{value:"Limiting the number of concurrent tasks",id:"limiting-the-number-of-concurrent-tasks",level:3},{value:"Deploy-time parameters",id:"deploy-time-parameters",level:3},{value:"Scheduling a flow",id:"scheduling-a-flow",level:2},{value:"Reproducing failed production runs",id:"reproducing-failed-production-runs",level:2},{value:"Staging flows for production",id:"staging-flows-for-production",level:3}],p={toc:u};function c(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,o.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"scheduling-metaflow-flows-with-apache-airflow"},"Scheduling Metaflow Flows with Apache Airflow"),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://airflow.apache.org/"},"Apache Airflow")," is a popular open-source workflow\norchestrator. It has a number of limitations compared to ",(0,a.kt)("a",{parentName:"p",href:"/production/scheduling-metaflow-flows/scheduling-with-argo-workflows"},"Argo\nWorkflows")," and\n",(0,a.kt)("a",{parentName:"p",href:"/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions"},"AWS Step\nFunctions"),", so\nwe mainly recommend it if you are an existing Airflow user and you want to avoid\nintroducing a new orchestrator in your environment."),(0,a.kt)("p",null,"The Metaflow-Airflow integration is a great way to modernize your Airflow deployment. It\nprovides a more user-friendly and productive development API for data scientists and\ndata engineers, without needing to change anything in your existing pipelines or\noperational playbooks, as described in ",(0,a.kt)("a",{parentName:"p",href:"https://outerbounds.com/blog/better-airflow-with-metaflow/"},"its announcement blog\npost"),". To learn how to\ndeploy and operate the integration, see ",(0,a.kt)("a",{parentName:"p",href:"https://outerbounds.com/engineering/operations/airflow/"},"Using Airflow with\nMetaflow"),"."),(0,a.kt)("p",null,"Here are the main benefits of using Metaflow with Airflow:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"You get to use the human-friendly API of Metaflow to define and test workflows.\nAlmost all features of Metaflow work with Airflow out of the box, except nested\nforeaches, which are not supported by Airflow, and ",(0,a.kt)("inlineCode",{parentName:"li"},"@batch")," as the current\nintegration only supports ",(0,a.kt)("inlineCode",{parentName:"li"},"@kubernetes"),"."),(0,a.kt)("li",{parentName:"ul"},"You can deploy Metaflow flows to your existing Airflow server without having to\nchange anything operationally. From the Airflow's point of view, Metaflow flows look\nlike any other Airflow DAG."),(0,a.kt)("li",{parentName:"ul"},"If you want to consider moving to another orchestrator supported by Metaflow, you can\ntest them easily just by changing one command to deploy to ",(0,a.kt)("a",{parentName:"li",href:"/production/scheduling-metaflow-flows/scheduling-with-argo-workflows"},"Argo\nWorkflows")," or\n",(0,a.kt)("a",{parentName:"li",href:"/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions"},"AWS Step\nFunctions"),".")),(0,a.kt)("p",null,"When running on Airflow, Metaflow code works exactly as it does locally: No changes are\nrequired in the code. All data artifacts produced by steps run on Airflow are available\nusing the ",(0,a.kt)("a",{parentName:"p",href:"/metaflow/client"},"Client API"),". All tasks are run on Kubernetes\nrespecting the ",(0,a.kt)("inlineCode",{parentName:"p"},"@resources")," decorator, as if the ",(0,a.kt)("inlineCode",{parentName:"p"},"@kubernetes")," decorator was added to\nall steps, as explained in ",(0,a.kt)("a",{parentName:"p",href:"/scaling/remote-tasks/requesting-resources"},"Executing Tasks\nRemotely"),"."),(0,a.kt)("p",null,"This document describes the basics of Airflow scheduling. If your project involves\nmultiple people, multiple workflows, or it is becoming business-critical, check out the\nsection around ",(0,a.kt)("a",{parentName:"p",href:"/production/coordinating-larger-metaflow-projects"},"coordinating larger Metaflow\nprojects"),"."),(0,a.kt)("admonition",{title:"Note",type:"info"},(0,a.kt)("p",{parentName:"admonition"},(0,a.kt)("a",{parentName:"p",href:"/metaflow/basics#conditionals"},"Conditional and recursive steps"),"\nintroduced in Metaflow 2.18, are not yet supported\non Airflow deployments. Contact ",(0,a.kt)("a",{parentName:"p",href:"http://slack.outerbounds.co"},"the Metaflow Slack")," if\nyou have a use case for this feature.")),(0,a.kt)("h2",{id:"pushing-a-flow-to-production"},"Pushing a flow to production"),(0,a.kt)("p",null,"Let's use ",(0,a.kt)("a",{parentName:"p",href:"../../metaflow/basics#how-to-define-parameters-for-flows"},"the flow from the section about\nparameters")," as an example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import FlowSpec, Parameter, step\n\nclass ParameterFlow(FlowSpec):\n    alpha = Parameter('alpha',\n                      help='Learning rate',\n                      default=0.01)\n\n    @step\n    def start(self):\n        print('alpha is %f' % self.alpha)\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print('alpha is still %f' % self.alpha)\n\nif __name__ == '__main__':\n    ParameterFlow()\n")),(0,a.kt)("p",null,"Save this script to a file ",(0,a.kt)("inlineCode",{parentName:"p"},"parameter_flow.py"),". To deploy a version to Airflow, simply\nrun"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"python parameter_flow.py --with retry airflow create parameter_dag.py \n")),(0,a.kt)("p",null,"This command takes a snapshot of your code in the working directory, as well as the\nversion of Metaflow used, and creates an Airflow DAG in ",(0,a.kt)("inlineCode",{parentName:"p"},"parameter_dag.py")," for\nscheduling on Airflow. You should deploy ",(0,a.kt)("inlineCode",{parentName:"p"},"parameter_dag.py")," to your Airflow instance\nlike you would do with any other user-written DAG."),(0,a.kt)("p",null,"Metaflow automatically maps the Parameters of your flow to corresponding parameters on\nAirflow. You can execute your Metaflow flow deployed on Airflow like any other Airflow\nDAG - seamlessly getting all the benefits of Airflow alongside all the benefits of\nMetaflow."),(0,a.kt)("h3",{id:"hardening-deployments"},"Hardening deployments"),(0,a.kt)("p",null,"It is highly recommended that you ",(0,a.kt)("a",{parentName:"p",href:"../../scaling/failures#retrying-tasks-with-the-retry-decorator"},"enable\nretries")," when deploying\nto Airflow, which you can do easily with ",(0,a.kt)("inlineCode",{parentName:"p"},"--with retry")," as shown above. However, make\nsure that all your steps are safe to retry before you do this. If some of your steps\ninteract with external services in ways that can't tolerate automatic retries, decorate\nthem with retry with times set to zero ","(","times=0",")"," as described in ",(0,a.kt)("a",{parentName:"p",href:"../../scaling/failures#how-to-prevent-retries"},"How to Prevent\nRetries"),"."),(0,a.kt)("p",null,"If you want to test on Airflow without interfering with a production flow, you can\nchange the name of your class, e.g. from ",(0,a.kt)("inlineCode",{parentName:"p"},"ParameterFlow")," to ",(0,a.kt)("inlineCode",{parentName:"p"},"ParameterFlowStaging"),", and\nairflow create the dag under a new name or ",(0,a.kt)("a",{parentName:"p",href:"/production/coordinating-larger-metaflow-projects"},"use the @project\ndecorator"),"."),(0,a.kt)("p",null,"Note that airflow create creates a new isolated production namespace for your production\nflow. Read ",(0,a.kt)("a",{parentName:"p",href:"/scaling/tagging"},"Organizing Results")," to learn all about namespace behavior."),(0,a.kt)("h3",{id:"limiting-the-number-of-concurrent-tasks"},"Limiting the number of concurrent tasks"),(0,a.kt)("p",null,"By default, Metaflow configures Airflow to execute at most 100 tasks concurrently within\na foreach step. This should ensure that most workflows finish quickly without\noverwhelming your Kubernetes cluster, the execution backend."),(0,a.kt)("p",null,"If your workflow includes a large foreach and you need results faster, you can increase\nthe default with the ",(0,a.kt)("inlineCode",{parentName:"p"},"--max-workers")," option. For instance, ",(0,a.kt)("inlineCode",{parentName:"p"},"airflow create --max-workers\n500")," allows 500 tasks to be executed concurrently for every foreach step."),(0,a.kt)("p",null,"This option is similar to ",(0,a.kt)("a",{parentName:"p",href:"/scaling/remote-tasks/controlling-parallelism"},(0,a.kt)("inlineCode",{parentName:"a"},"run\n--max-workers"))," that is used to\nlimit concurrency outside Airflow."),(0,a.kt)("h3",{id:"deploy-time-parameters"},"Deploy-time parameters"),(0,a.kt)("p",null,"You can customize Airflow deployments through Parameters that are evaluated at the\ndeployment time, i.e. when ",(0,a.kt)("inlineCode",{parentName:"p"},"airflow create")," is executed."),(0,a.kt)("p",null,"For instance, you can change the default value of a ",(0,a.kt)("inlineCode",{parentName:"p"},"Parameter")," based on who deployed\nthe workflow or what Git branch the deployment was executed in. Crucially, the function\nin Parameter is evaluated only once during ",(0,a.kt)("inlineCode",{parentName:"p"},"airflow create")," and not during the execution\nof the flow."),(0,a.kt)("p",null,"You can run the flow locally as usual. The function inside ",(0,a.kt)("inlineCode",{parentName:"p"},"Parameter")," is called only\nonce when the execution starts."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import FlowSpec, Parameter, step, JSONType\nfrom datetime import datetime\nimport json\n\ndef deployment_info(context):\n    return json.dumps({'who': context.user_name,\n                       'when': datetime.now().isoformat()})\n\nclass DeploymentInfoFlow(FlowSpec):\n    info = Parameter('deployment_info',\n                     type=JSONType,\n                     default=deployment_info)\n\n    @step\n    def start(self):\n        print('This flow was deployed at %s by %s'\\\n              % (self.info['when'], self.info['who']))\n        self.next(self.end)\n\n    @step\n    def end(self):\n        pass\n\nif __name__ == '__main__':\n    DeploymentInfoFlow()\n")),(0,a.kt)("p",null,"When ",(0,a.kt)("inlineCode",{parentName:"p"},"airflow create")," is called, ",(0,a.kt)("inlineCode",{parentName:"p"},"deployment_info")," is evaluated which captures your\nusername and the time of deployment. This information remains constant on Airflow\nWorkflows, although the user may override the default value."),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"context")," object is passed to any function defined in Parameter. It contains various\nfields related to the flow being deployed. By relying on the values passed in context,\nyou can create generic deploy-time functions that can be reused by multiple flows."),(0,a.kt)("h2",{id:"scheduling-a-flow"},"Scheduling a flow"),(0,a.kt)("p",null,"By default, a flow on Airflow does not run automatically. You need to set up a trigger\nto launch the flow when an event occurs."),(0,a.kt)("p",null,"On Airflow, Metaflow provides built-in support for triggering Metaflow flows\nthrough time-based ","(","cron",")"," triggers, which, as the name implies, triggers the\nworkflow at a certain time. As of today, ","[event-based triggering]","\n(/production/event-triggering) is not supported on Airflow."),(0,a.kt)("p",null,"Time-based triggers are implemented at the FlowSpec-level using the ",(0,a.kt)("inlineCode",{parentName:"p"},"@schedule"),"\ndecorator. This flow is triggered hourly:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from metaflow import FlowSpec, schedule, step\nfrom datetime import datetime\n\n@schedule(hourly=True)\nclass HourlyFlow(FlowSpec):\n\n    @step\n    def start(self):\n        now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        print('time is %s' % now)\n        self.next(self.end)\n\n    @step\n    def end(self):\n        pass\n\nif __name__ == '__main__':\n    HourlyFlow()\n")),(0,a.kt)("p",null,"You can define the schedule with ",(0,a.kt)("inlineCode",{parentName:"p"},"@schedule")," in one of the following ways:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"@schedule(weekly=True)")," runs the workflow on Sundays at midnight."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"@schedule(daily=True)")," runs the workflow every day at midnight."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"@schedule(hourly=True)")," runs the workflow every hour."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"@schedule(cron='0 10 * * ? *')")," runs the workflow at the given\n",(0,a.kt)("a",{parentName:"li",href:"http://en.wikipedia.org/wiki/cron"},"Cron")," schedule, in this case at 10am UTC every\nday.")),(0,a.kt)("h2",{id:"reproducing-failed-production-runs"},"Reproducing failed production runs"),(0,a.kt)("p",null,"Let's use ",(0,a.kt)("a",{parentName:"p",href:"/metaflow/debugging#how-to-use-the-resume-command"},(0,a.kt)("inlineCode",{parentName:"a"},"DebugFlow")," from the debugging\nsection")," as an example. The flow\ncontains a bug in the step ",(0,a.kt)("inlineCode",{parentName:"p"},"b"),". When you run it, the failed run will look like this on\nthe Airflow UI:"),(0,a.kt)("p",null,(0,a.kt)("img",{src:n(2569).Z,width:"800",height:"372"})),(0,a.kt)("p",null,"The graph visualization shows that step b failed, as expected. First, you should inspect\nthe logs of the failed step in the Airflow UI (or the Metaflow UI) to get an idea of why\nit failed."),(0,a.kt)("p",null,"Notice the Metaflow Run ID of ",(0,a.kt)("inlineCode",{parentName:"p"},"airflow-ec19e85042a1")," that is available from the Rendered\nTemplate page for the failed task in the Airflow UI (look for the ",(0,a.kt)("inlineCode",{parentName:"p"},"metaflow_run_id"),"\nattribute). You can use this Run ID to locate the execution in the Metaflow UI as well\nif needed."),(0,a.kt)("p",null,(0,a.kt)("img",{src:n(7861).Z,width:"1641",height:"891"})),(0,a.kt)("p",null,"Next, we want to reproduce the above error locally. We do this by resuming the specific\nAirflow run that failed:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"python debug.py resume --origin-run-id airflow-ec19e85042a1\n")),(0,a.kt)("p",null,"This will reuse the results of the start and a step from the Airflow run. It will try to\nrerun the step ",(0,a.kt)("inlineCode",{parentName:"p"},"b")," locally, which fails with the same error as it does in production."),(0,a.kt)("p",null,"You can fix the error locally, as above. In the case of this simple flow, you can run\nthe whole flow locally to confirm that the fix works. After validating the results, you\nwould deploy a new version to production with airflow create."),(0,a.kt)("p",null,"However, this might not be a feasible approach for complex production flow. For\ninstance, the flow might process large amounts of data that can not be handled in your\nlocal instance. We have better approaches for staging flows for production:"),(0,a.kt)("h3",{id:"staging-flows-for-production"},"Staging flows for production"),(0,a.kt)("p",null,"The easiest approach to test a demanding flow is to run it on Kubernetes. This works\neven with resume:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"python debug.py resume --origin-run-id airflow-ec19e85042a1 --with kubernetes\n")),(0,a.kt)("p",null,"This will resume your flow and run every step on Kubernetes. When you are ready to test\na fixed flow end-to-end, just run it as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"python debug.py run --with kubernetes\n")),(0,a.kt)("p",null,"Alternatively, you can change the name of the flow temporarily, e.g. from DebugFlow to\nDebugFlowStaging. Then you can run ",(0,a.kt)("inlineCode",{parentName:"p"},"airflow create")," with the new name, which will create\na separate staging flow on Airflow. You can also use the\n",(0,a.kt)("a",{parentName:"p",href:"/production/coordinating-larger-metaflow-projects#the-project-decorator"},(0,a.kt)("inlineCode",{parentName:"a"},"@project")),"\ndecorator."),(0,a.kt)("p",null,"You can test the staging flow freely without interfering with the production flow. Once\nthe staging flow runs successfully, you can confidently deploy a new version to\nproduction."))}c.isMDXComponent=!0},2569:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/airflow_debug-441737d0b84119026ed968328e25fc77.png"},7861:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/airflow_debug1-8830a34e874a904add50e49766f0d5a3.png"}}]);